{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.optimize import curve_fit\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "# 归一化函数\n",
    "def normalize(tokens):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "    tokens = np.array(tokens).reshape(-1, 1)\n",
    "    return scaler.fit_transform(tokens).flatten()\n",
    "\n",
    "# 三次函数模型\n",
    "def cubic_model(x, a, b, c, d):\n",
    "    return a * x**3 + b * x**2 + c * x + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../count_datas/tokens_train.json', 'r') as f:\n",
    "    res_tokens = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../count_datas/tokens_infer.json', 'r') as f:\n",
    "    res_tokens_infer = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../count_datas/f1.json', 'r') as f:\n",
    "    res_f1 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res(out_name, type_str, keys, train_keys, valid_key,fig_out_name):\n",
    "    tmp_res = {}\n",
    "    tmp_res['train'] = {}\n",
    "    tmp_res['valid'] = {}\n",
    "    for key in keys:\n",
    "        now_tokens = res_tokens[key]\n",
    "        now_tokens_infer = res_tokens_infer[key]\n",
    "        now_f1 = res_f1[key]\n",
    "\n",
    "        need_keys = [\n",
    "            ['{}_bidirection_A','{}_pipeline_A',],\n",
    "            ['{}_bidirection_B','{}_pipeline_B',],\n",
    "            ['{}_bidirection_C','{}_pipeline_C',],\n",
    "            ['{}_bidirection_D','{}_pipeline_D',],\n",
    "            ['{}_bidirection_E','{}_pipeline_E',],\n",
    "        ]\n",
    "        processed_tmp_f1 = []\n",
    "        processed_tmp_tokens = []\n",
    "        processed_tmp_tokens_infer = []\n",
    "        for need_key_list in need_keys:\n",
    "            val_f1 = 0\n",
    "            val_tokens = 0\n",
    "            val_tokens_infer = 0\n",
    "            for need_key in need_key_list:\n",
    "                val_f1 += float(now_f1['{}_A'.format(need_key.format(type_str))][-1])\n",
    "                val_tokens += now_tokens[need_key.format(type_str)]\n",
    "                val_tokens_infer += now_tokens_infer[need_key.format(type_str)]\n",
    "            processed_tmp_f1.append(val_f1/2)\n",
    "            processed_tmp_tokens.append(val_tokens/2)\n",
    "            processed_tmp_tokens_infer.append(val_tokens_infer/2)\n",
    "\n",
    "\n",
    "        # print('处理前:{}'.format(processed_tmp_tokens))\n",
    "        sum_val = 0\n",
    "        for i in [1, 3, 4]:\n",
    "            sum_val += processed_tmp_tokens_infer[i]\n",
    "        # print('sum_val:{}'.format(sum_val))\n",
    "        sum_val = sum_val/3\n",
    "        # print('sum_val:{}'.format(sum_val))\n",
    "        portions = []\n",
    "        for i in [1, 3, 4]:\n",
    "            portion = (processed_tmp_tokens_infer[i]/sum_val - 1)*1 +1\n",
    "            portions.append(portion)\n",
    "            processed_tmp_tokens[i] *= portion\n",
    "        for i in range(len(processed_tmp_tokens)):\n",
    "            processed_tmp_tokens[i] = int(processed_tmp_tokens[i])\n",
    "        # 输出调整后的processed_tmp_tokens\n",
    "        # print('portions:{}'.format(portions))\n",
    "        # print(processed_tmp_tokens)\n",
    "\n",
    "        y_ref = (float(now_f1['{}_v0_A'.format(type_str)][-1]) + float(now_f1['{}_pipeline_A_A'.format(type_str)][-1]) + float(now_f1['{}_bidirection_A_A'.format(type_str)][-1])) / 3\n",
    "        # index_list = np.argsort(processed_tmp_tokens)\n",
    "        index_list = [0,1,2,3,4]\n",
    "        processed_tmp_tokens = [processed_tmp_tokens[i] for i in index_list]\n",
    "        processed_tmp_f1 = [processed_tmp_f1[i] for i in index_list]\n",
    "        if key in train_keys:\n",
    "            tmp_res['train']['{}'.format(key)] = {\n",
    "                'f1':processed_tmp_f1,\n",
    "                'tokens':processed_tmp_tokens,\n",
    "                'tokens_infer':processed_tmp_tokens_infer,\n",
    "                'portions': portions,\n",
    "                'y_ref': y_ref\n",
    "            }\n",
    "        else:\n",
    "            tmp_res['valid']['{}'.format(key)] = {\n",
    "                'f1':processed_tmp_f1,\n",
    "                'tokens':processed_tmp_tokens,\n",
    "                'tokens_infer':processed_tmp_tokens_infer,\n",
    "                'portions': portions,\n",
    "                'y_ref': y_ref\n",
    "            }\n",
    "\n",
    "    all_tokens = []\n",
    "    all_f1 = []\n",
    "    all_portions = []\n",
    "    for key in train_keys:\n",
    "        tokens = tmp_res['train']['{}'.format(key)]['tokens']\n",
    "        y_ref = tmp_res['train']['{}'.format(key)]['y_ref']\n",
    "        f1 = tmp_res['train']['{}'.format(key)]['f1']\n",
    "        portions = tmp_res['train']['{}'.format(key)]['portions']\n",
    "        all_portions.append(portions)\n",
    "        # print(key)\n",
    "        # print('tokens:{}'.format(tokens))\n",
    "        # print('y_ref:{}'.format(y_ref))\n",
    "        # print('f1:{}'.format(f1))\n",
    "        # print('portions:{}'.format(portions))\n",
    "        tokens_normalized = normalize(tokens)\n",
    "        f1_adjusted = np.array(f1) - y_ref  # Adjust y by subtracting y_ref\n",
    "        all_tokens.extend(tokens_normalized)\n",
    "        all_f1.extend(f1_adjusted)\n",
    "\n",
    "    all_tokens = np.array(all_tokens)\n",
    "    all_f1 = np.array(all_f1)\n",
    "    # 使用三次函数进行拟合\n",
    "    popt_cubic, _ = curve_fit(cubic_model, all_tokens, all_f1, p0=(1, 1, 1, 0))\n",
    "\n",
    "    # 绘制拟合曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(all_tokens, all_f1, color='blue', label='Data')\n",
    "    plt.plot(np.linspace(0, 100, 500), cubic_model(np.linspace(0, 100, 500), *popt_cubic), color='red', label='Fitted cubic curve')\n",
    "    plt.title('Curve Fitting for F1 vs Tokens (Adjusted by y_ref) - Cubic Model')\n",
    "    plt.xlabel('Normalized Tokens')\n",
    "    plt.ylabel('Adjusted F1')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('./fig_拟合算力/{}.png'.format(fig_out_name), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    a,b,c,d = popt_cubic\n",
    "    average_portions = [sum(x[i] for x in all_portions) / len(all_portions) for i in range(len(all_portions[0]))]\n",
    "    average_portions = [1,average_portions[0],1,average_portions[1],average_portions[2]]\n",
    "    valid_res = []\n",
    "    valid_tokens = tmp_res['valid']['{}'.format(valid_key)]['tokens']\n",
    "    valid_y_ref = tmp_res['valid']['{}'.format(valid_key)]['y_ref']\n",
    "\n",
    "    valid_tokens_normalized = normalize(valid_tokens)\n",
    "    print('valid_tokens_normalized:{}'.format(valid_tokens_normalized))\n",
    "    print('valid_y_ref:{}'.format(valid_y_ref))\n",
    "    for token in valid_tokens_normalized:\n",
    "        print('token:{}'.format(token))\n",
    "        print('cubic_model(token,a,b,c,d):{}'.format(cubic_model(token,a,b,c,d)))\n",
    "        valid_res.append(cubic_model(token,a,b,c,d)+valid_y_ref)\n",
    "    tmp_res['valid']['valid_res'] = valid_res\n",
    "    tmp_res['train']['xishu'] = list(popt_cubic)\n",
    "    print('valid_res:{}'.format(valid_res))\n",
    "    print('原始valid_res:{}'.format(tmp_res['valid']['{}'.format(valid_key)]['f1']))\n",
    "    return tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "out_name = 'CMeIE'\n",
    "type_str = 'withtype'\n",
    "keys = [\n",
    "    'CMeIE-glm4',\n",
    "    'CMeIE-qwen2.5',\n",
    "    'CMeIE-baichuan2',\n",
    "    'CMeIE-intern2.5',\n",
    "]\n",
    "train_keys = [\n",
    "    'CMeIE-glm4',\n",
    "    'CMeIE-qwen2.5',\n",
    "    'CMeIE-baichuan2',\n",
    "]\n",
    "valid_key = 'CMeIE-intern2.5'\n",
    "fig_out_name = 'CMeIE---withtype'\n",
    "tmp_res = get_res(out_name, type_str, keys, train_keys, valid_key, fig_out_name)\n",
    "res['CMeIE---带实体类型'] = tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_name = 'CMeIE'\n",
    "type_str = 'notype'\n",
    "fig_out_name = 'CMeIE---notype'\n",
    "tmp_res = get_res(out_name, type_str, keys, train_keys, valid_key, fig_out_name)\n",
    "res['CMeIE---不带实体类型'] = tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_name = 'ADE'\n",
    "type_str = 'withtype'\n",
    "keys = [\n",
    "    'ADE-glm4',\n",
    "    'ADE-qwen2.5',\n",
    "    'ADE-llama3.1',\n",
    "    'ADE-deepseek',\n",
    "]\n",
    "train_keys = [\n",
    "    'ADE-qwen2.5',\n",
    "    'ADE-glm4',\n",
    "    'ADE-deepseek',   \n",
    "]\n",
    "valid_key = 'ADE-llama3.1'\n",
    "fig_out_name = 'ADE---withtype'\n",
    "tmp_res = get_res(out_name, type_str, keys, train_keys, valid_key, fig_out_name)\n",
    "res['ADE---带实体类型'] = tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_name = 'ADE'\n",
    "type_str = 'notype'\n",
    "fig_out_name = 'ADE---notype'\n",
    "tmp_res = get_res(out_name, type_str, keys, train_keys, valid_key, fig_out_name)\n",
    "res['ADE---不带实体类型'] = tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('拟合算力_f1.json', 'w') as f:\n",
    "    json.dump(res, f, ensure_ascii = False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
